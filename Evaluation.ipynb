{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b67476e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\aruni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aruni\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.spatial.distance import cosine\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import string\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6216b462",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc20d7e",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f811b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics functions : rmse and pearson correlation\n",
    "def RMSE(actual, pred):\n",
    "    return sqrt(mean_squared_error(actual, pred))\n",
    "\n",
    "def Pearson(actual,pred):\n",
    "    mean_a = sum(actual) / len(actual)\n",
    "    mean_p = sum(pred) / len(pred)\n",
    "    cov = sum((a - mean_a) * (b - mean_p) for (a, b) in zip(actual, pred)) / len(actual)\n",
    "    p = float(cov / (np.std(actual) * np.std(pred)))\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93830218",
   "metadata": {},
   "source": [
    "## Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce7dadcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg(xTrain,yTrain,xTest):\n",
    "    model = LinearRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    y_pred = np.clip(y_pred, 0, 5)\n",
    "    return y_pred\n",
    "\n",
    "def isotonic_reg(xTrain,yTrain,xTest):\n",
    "    model = IsotonicRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    y_pred = np.clip(y_pred, 0, 5)\n",
    "    return y_pred\n",
    "\n",
    "def ridge_reg(xTrain, yTrain, xTest):\n",
    "    model = Ridge()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    y_pred = model.predict(xTest)\n",
    "    y_pred = np.clip(y_pred, 0, 5)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22f8ef",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "The similarity scores are used as data to train different regression models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "195938b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg(l):\n",
    "    return sum(l) / len(l)\n",
    "\n",
    "def check_nan(arr):\n",
    "    idx_NaN = np.isnan(arr)\n",
    "    arr[idx_NaN] = 0\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "181f86db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_models(mdl, df):\n",
    "    mdl_score = 'normalized_' + mdl + '_similarity_score'\n",
    "    X=df[[mdl_score,'keyword_match','normalized_length_ratio']]\n",
    "    X_iso=df[mdl_score]\n",
    "    y=df['score_avg']\n",
    "    \n",
    "    xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "    xTrain_i, xTest_i, yTrain_i, yTest_i = train_test_split(X_iso, y, test_size=0.3, random_state=101)\n",
    "\n",
    "\n",
    "    linear_yPred = [float(x) for x in linear_reg(xTrain,yTrain,xTest)]\n",
    "    ridge_yPred = [float(x) for x in ridge_reg(xTrain,yTrain,xTest)]\n",
    "    isotonic_yPred = list(np.nan_to_num(isotonic_reg(xTrain_i,yTrain_i,xTest_i), nan=0))\n",
    "\n",
    "    y = check_nan(np.asarray(yTest))\n",
    "    y_iso = check_nan(np.asarray(yTest_i))\n",
    "    isotonic_pred = check_nan(np.asarray([round(i*2)/2  for i in isotonic_yPred]))\n",
    "    linear_pred = check_nan(np.asarray([round(i*2)/2  for i in linear_yPred]))\n",
    "    ridge_pred = check_nan(np.asarray([round(i*2)/2  for i in ridge_yPred]))\n",
    "\n",
    "    return RMSE(y_iso,isotonic_pred), Pearson(y_iso,isotonic_pred), RMSE(y,linear_pred), Pearson(y,linear_pred), RMSE(y,ridge_pred), Pearson(y,ridge_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc35530",
   "metadata": {},
   "source": [
    "## Evaluation of each model for the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e6256bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_results(dataset):\n",
    "    df = pd.read_csv(dataset)\n",
    "    print(\"Evaluation Results:\")\n",
    "    models=[\"bert\",\"elmo\",\"gpt\",\"gpt2\",\"universal\", \"roberta\",\"xlnet\"]\n",
    "\n",
    "    for m in models:\n",
    "        print(m.upper())\n",
    "        rmse_iso,rmse_lin,rmse_rid = [],[],[]\n",
    "        pc_iso, pc_lin, pc_rid = [], [],[]\n",
    "\n",
    "        for i in range(0, 1000):\n",
    "            iso_rmse_score, iso_pc_score, lin_rmse_score, lin_pc_score, rid_rmse_score, rid_pc_score = evaluate_models(m,df)\n",
    "            rmse_iso.append(iso_rmse_score)\n",
    "            pc_iso.append(iso_pc_score)\n",
    "\n",
    "            rmse_lin.append(lin_rmse_score)\n",
    "            pc_lin.append(lin_pc_score)\n",
    "\n",
    "            rmse_rid.append(rid_rmse_score)\n",
    "            pc_rid.append(rid_pc_score)\n",
    "\n",
    "\n",
    "        print(\"Isotonic Regression \\t ==> \\t RMSE :\",round(avg(rmse_iso), 3),\" \\t Pearson Correlation :\", round(avg(pc_iso), 3))\n",
    "        print(\"Linear Regression \\t ==> \\t RMSE :\",round(avg(rmse_lin), 3),\" \\t Pearson Correlation :\", round(avg(pc_lin), 3))\n",
    "        print(\"Ridge Regression \\t ==> \\t RMSE :\",round(avg(rmse_rid), 3),\" \\t Pearson Correlation :\", round(avg(pc_rid), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a359938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Dataset 1\n",
      "Evaluation Results:\n",
      "BERT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.072  \t Pearson Correlation : 0.314\n",
      "Linear Regression \t ==> \t RMSE : 1.013  \t Pearson Correlation : 0.428\n",
      "Ridge Regression \t ==> \t RMSE : 1.015  \t Pearson Correlation : 0.425\n",
      "ELMO\n",
      "Isotonic Regression \t ==> \t RMSE : 1.001  \t Pearson Correlation : 0.452\n",
      "Linear Regression \t ==> \t RMSE : 0.961  \t Pearson Correlation : 0.525\n",
      "Ridge Regression \t ==> \t RMSE : 0.968  \t Pearson Correlation : 0.513\n",
      "GPT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.079  \t Pearson Correlation : 0.279\n",
      "Linear Regression \t ==> \t RMSE : 1.024  \t Pearson Correlation : 0.412\n",
      "Ridge Regression \t ==> \t RMSE : 1.021  \t Pearson Correlation : 0.418\n",
      "GPT2\n",
      "Isotonic Regression \t ==> \t RMSE : 1.093  \t Pearson Correlation : 0.226\n",
      "Linear Regression \t ==> \t RMSE : 1.041  \t Pearson Correlation : 0.379\n",
      "Ridge Regression \t ==> \t RMSE : 1.043  \t Pearson Correlation : 0.375\n",
      "UNIVERSAL\n",
      "Isotonic Regression \t ==> \t RMSE : 0.972  \t Pearson Correlation : 0.508\n",
      "Linear Regression \t ==> \t RMSE : 0.913  \t Pearson Correlation : 0.586\n",
      "Ridge Regression \t ==> \t RMSE : 0.918  \t Pearson Correlation : 0.581\n",
      "ROBERTA\n",
      "Isotonic Regression \t ==> \t RMSE : 1.099  \t Pearson Correlation : 0.238\n",
      "Linear Regression \t ==> \t RMSE : 1.022  \t Pearson Correlation : 0.411\n",
      "Ridge Regression \t ==> \t RMSE : 1.027  \t Pearson Correlation : 0.402\n",
      "XLNET\n",
      "Isotonic Regression \t ==> \t RMSE : 1.127  \t Pearson Correlation : 0.158\n",
      "Linear Regression \t ==> \t RMSE : 1.043  \t Pearson Correlation : 0.375\n",
      "Ridge Regression \t ==> \t RMSE : 1.051  \t Pearson Correlation : 0.353\n"
     ]
    }
   ],
   "source": [
    "print('Results for Dataset 1')    \n",
    "eval_results('Dataset 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "149e5c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Dataset 2\n",
      "Evaluation Results:\n",
      "BERT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.478  \t Pearson Correlation : 0.563\n",
      "Linear Regression \t ==> \t RMSE : 1.381  \t Pearson Correlation : 0.645\n",
      "Ridge Regression \t ==> \t RMSE : 1.387  \t Pearson Correlation : 0.642\n",
      "ELMO\n",
      "Isotonic Regression \t ==> \t RMSE : 1.539  \t Pearson Correlation : 0.513\n",
      "Linear Regression \t ==> \t RMSE : 1.421  \t Pearson Correlation : 0.615\n",
      "Ridge Regression \t ==> \t RMSE : 1.44  \t Pearson Correlation : 0.601\n",
      "GPT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.608  \t Pearson Correlation : 0.447\n",
      "Linear Regression \t ==> \t RMSE : 1.427  \t Pearson Correlation : 0.611\n",
      "Ridge Regression \t ==> \t RMSE : 1.452  \t Pearson Correlation : 0.593\n",
      "GPT2\n",
      "Isotonic Regression \t ==> \t RMSE : 1.617  \t Pearson Correlation : 0.432\n",
      "Linear Regression \t ==> \t RMSE : 1.441  \t Pearson Correlation : 0.6\n",
      "Ridge Regression \t ==> \t RMSE : 1.423  \t Pearson Correlation : 0.622\n",
      "UNIVERSAL\n",
      "Isotonic Regression \t ==> \t RMSE : 1.491  \t Pearson Correlation : 0.558\n",
      "Linear Regression \t ==> \t RMSE : 1.357  \t Pearson Correlation : 0.661\n",
      "Ridge Regression \t ==> \t RMSE : 1.357  \t Pearson Correlation : 0.664\n",
      "ROBERTA\n",
      "Isotonic Regression \t ==> \t RMSE : 1.526  \t Pearson Correlation : 0.526\n",
      "Linear Regression \t ==> \t RMSE : 1.371  \t Pearson Correlation : 0.647\n",
      "Ridge Regression \t ==> \t RMSE : 1.382  \t Pearson Correlation : 0.639\n",
      "XLNET\n",
      "Isotonic Regression \t ==> \t RMSE : 1.683  \t Pearson Correlation : 0.349\n",
      "Linear Regression \t ==> \t RMSE : 1.467  \t Pearson Correlation : 0.595\n",
      "Ridge Regression \t ==> \t RMSE : 1.506  \t Pearson Correlation : 0.569\n"
     ]
    }
   ],
   "source": [
    "print('Results for Dataset 2')    \n",
    "eval_results('Dataset 2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc3d943d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Dataset 3\n",
      "Evaluation Results:\n",
      "BERT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.289  \t Pearson Correlation : 0.659\n",
      "Linear Regression \t ==> \t RMSE : 1.122  \t Pearson Correlation : 0.731\n",
      "Ridge Regression \t ==> \t RMSE : 1.074  \t Pearson Correlation : 0.769\n",
      "ELMO\n",
      "Isotonic Regression \t ==> \t RMSE : 1.067  \t Pearson Correlation : 0.771\n",
      "Linear Regression \t ==> \t RMSE : 1.086  \t Pearson Correlation : 0.76\n",
      "Ridge Regression \t ==> \t RMSE : 1.125  \t Pearson Correlation : 0.743\n",
      "GPT\n",
      "Isotonic Regression \t ==> \t RMSE : 1.217  \t Pearson Correlation : 0.684\n",
      "Linear Regression \t ==> \t RMSE : 1.181  \t Pearson Correlation : 0.697\n",
      "Ridge Regression \t ==> \t RMSE : 1.142  \t Pearson Correlation : 0.738\n",
      "GPT2\n",
      "Isotonic Regression \t ==> \t RMSE : 1.528  \t Pearson Correlation : 0.465\n",
      "Linear Regression \t ==> \t RMSE : 1.227  \t Pearson Correlation : 0.664\n",
      "Ridge Regression \t ==> \t RMSE : 1.195  \t Pearson Correlation : 0.7\n",
      "UNIVERSAL\n",
      "Isotonic Regression \t ==> \t RMSE : 1.126  \t Pearson Correlation : 0.738\n",
      "Linear Regression \t ==> \t RMSE : 1.134  \t Pearson Correlation : 0.724\n",
      "Ridge Regression \t ==> \t RMSE : 1.126  \t Pearson Correlation : 0.729\n",
      "ROBERTA\n",
      "Isotonic Regression \t ==> \t RMSE : 1.225  \t Pearson Correlation : 0.68\n",
      "Linear Regression \t ==> \t RMSE : 1.086  \t Pearson Correlation : 0.747\n",
      "Ridge Regression \t ==> \t RMSE : 1.122  \t Pearson Correlation : 0.732\n",
      "XLNET\n",
      "Isotonic Regression \t ==> \t RMSE : 1.24  \t Pearson Correlation : 0.693\n",
      "Linear Regression \t ==> \t RMSE : 1.123  \t Pearson Correlation : 0.735\n",
      "Ridge Regression \t ==> \t RMSE : 1.158  \t Pearson Correlation : 0.713\n"
     ]
    }
   ],
   "source": [
    "print('Results for Dataset 3')    \n",
    "eval_results('Dataset 3.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4cfe120",
   "metadata": {},
   "source": [
    "## Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0abcbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_reg_model(xTrain,yTrain):\n",
    "    model = LinearRegression()\n",
    "    model.fit(xTrain, yTrain)\n",
    "    with open('asag_model.pkl','wb') as f:\n",
    "        pickle.dump(model,f)\n",
    "    with open('clipping_params.pkl', 'wb') as f:\n",
    "        pickle.dump((0, 5), f)\n",
    "df=pd.read_csv('Dataset 1.csv')\n",
    "mdl_score = 'normalized_universal_similarity_score'\n",
    "X=df[[mdl_score,'keyword_match','normalized_length_ratio']]\n",
    "y=df['score_avg']\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=0.3, random_state=101)\n",
    "linear_reg_model(xTrain,yTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "722f258a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(q, ans):\n",
    "    q = q.lower()\n",
    "    ans = ans.lower()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    q_res = q.translate(str.maketrans('', '', string.punctuation))\n",
    "    ans_res = ans.translate(str.maketrans('', '', string.punctuation))\n",
    "    q_tokens = word_tokenize(q_res)\n",
    "    ans_tokens = word_tokenize(ans_res)\n",
    "    demoted_tokens = [t for t in ans_tokens if t not in q_tokens]\n",
    "    filtered_sent = [w for w in demoted_tokens if not w in stop_words]\n",
    "    return filtered_sent\n",
    "\n",
    "def check_tokens(sent):\n",
    "    if not list:\n",
    "        sent = word_tokenize(sent)\n",
    "    return sent\n",
    "\n",
    "def universal(sent):\n",
    "    module_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
    "    univ_model = hub.load(module_url)\n",
    "    tokens = check_tokens(sent)\n",
    "    embedding = tf.nn.l2_normalize(univ_model(tokens))\n",
    "    word_arr=[]\n",
    "    for i in range(len(embedding)):\n",
    "        word_arr.append(embedding[i].numpy())\n",
    "    return word_arr\n",
    "def preprocess_lr(ans):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    ans_res = ans.translate(str.maketrans('','',string.punctuation))\n",
    "    ans_tokens = word_tokenize(ans_res)\n",
    "    filtered_sent = [w for w in ans_tokens if not w in stop_words]\n",
    "    return filtered_sent\n",
    "\n",
    "def keyword_match(s_ans, d_ans):\n",
    "    d = Rake()\n",
    "    d.extract_keywords_from_text(d_ans)\n",
    "    da = d.get_ranked_phrases()\n",
    "    s = Rake()\n",
    "    s.extract_keywords_from_text(s_ans)\n",
    "    sa = s.get_ranked_phrases()\n",
    "    cnt = 0\n",
    "    for i in da:\n",
    "        if i in sa:\n",
    "            cnt += 1\n",
    "    frac = cnt/len(da)\n",
    "    return frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "0195931e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does the compiler handle inline functions?\n",
      "Answer: compiler ignores inline qualifier\n",
      "Desired Answer: It makes a copy of the function code in every place where a function call is made.\n",
      "Score: 2.564852370002992\n"
     ]
    }
   ],
   "source": [
    "with open('asag_model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "with open('clipping_params.pkl', 'rb') as f:\n",
    "    clip_params = pickle.load(f)\n",
    "q = input('Question: ')\n",
    "ans = input('Answer: ')\n",
    "model_ans = input('Desired Answer: ')\n",
    "no_ans=['', ' ', 'not answered']\n",
    "if ans.lower() in no_ans:\n",
    "    print('Score:',0)\n",
    "else:\n",
    "    #Similarity score\n",
    "    model_preproc = preprocessing(q, model_ans)\n",
    "    stu_preproc = preprocessing(q, ans)\n",
    "    if len(model_preproc) == 0:\n",
    "        model_preproc = model_ans.split()\n",
    "    if len(stu_preproc) == 0:\n",
    "        stu_preproc = ans.split()\n",
    "\n",
    "    model_arr=universal(model_preproc)\n",
    "    stu_arr=universal(stu_preproc)\n",
    "    sim_score=1-cosine(sum(model_arr),sum(stu_arr))\n",
    "    sim_scores=np.concatenate((np.array([sim_score]),np.array(df['universal_similarity_score'])),axis=None)\n",
    "    score_n = MinMaxScaler().fit_transform(sim_scores.reshape(-1,1))\n",
    "    \n",
    "    #Length Ratio\n",
    "    stu_preproc=preprocess_lr(ans)\n",
    "    model_preproc=preprocess_lr(model_ans)\n",
    "    if len(model_preproc) == 0:\n",
    "        model_preproc = model_ans.split()\n",
    "    if len(stu_preproc) == 0:\n",
    "        stu_preproc = ans.split()\n",
    "    lr=len(stu_preproc)/len(model_preproc)\n",
    "    lrs=np.concatenate((np.array([lr]),np.array(df['length_ratio'])),axis=None)\n",
    "    lrs = MinMaxScaler().fit_transform(lrs.reshape(-1,1))\n",
    "    \n",
    "    match_val=keyword_match(ans,model_ans)\n",
    "    \n",
    "    y_pred = model.predict([[score_n[0],match_val, lrs[0]]])\n",
    "    y_pred = np.clip(y_pred, clip_params[0], clip_params[1])\n",
    "\n",
    "    print('Score:',y_pred[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fyp",
   "language": "python",
   "name": "fyp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
